---
  title: HyC-LoRA:Memory Efficient LoRA Fine-tuning with Hybrid Activation Compression（MLSys 2025）
  date: 2025-02-12
  image:
    focal_point: 'top'

  subtitle:   
    接收会议： Machine Learning and System (MLSys) 2025
    <br>
    作者：王与进, 董枢楠, 黄宗乐, 尤忆晨，杨华中, 刘永攀, 贾弘洋
---


  为了减少缓存激活值的内存消耗，并进一步实现设备端内存高效的微调系统，我们提出了HyC-LoRA，这是一种基于混合压缩框架的LoRA训练方法变体，能够在所有算子中实现接近2比特的缓存激活值量化。

  <!--more-->

  大型语言模型（LLMs）被广泛应用于对话和文本摘要等场景中。随着对模型定制化和隐私保护需求的增加，针对大模型的轻量化微调方法开始受到广泛关注。低秩自适应（LoRA）是目前最广泛使用的微调算法之一，它在将预训练的大型语言模型迁移到下游任务时，显著减少了可调参数的数量以及相关的优化器内存占用。然而，过去的研究忽视了低秩自适应中缓存激活值的开销，导致系统内存使用效率未能达到最优。

  为了减少缓存激活值的内存消耗，并进一步实现设备端内存高效的微调系统，我们提出了HyC-LoRA，这是一种基于混合压缩框架的LoRA训练方法变体，能够在所有算子中实现接近2比特的缓存激活值量化。HyC-LoRA观察到，在LoRA微调过程中，用于反向传播的临时缓存激活值占据了内存消耗的主要部分，而非线性模块中的缓存激活值则是内存消耗的主要来源，其量化更具挑战性。基于此，HyC-LoRA提出了双层次的混合压缩机制：（1）\textbf{算子内混合压缩}：HyC-LoRA检测缓存激活值中的极端异常值，并通过结构化异常值存储来减少量化误差；（2）\textbf{算子间混合压缩}：HyC-LoRA利用LoRA适配器，通过算子间重排序和融合，实现量化误差补偿和选择性重计算。最后，HyC-LoRA实现了一个缓存激活值压缩系统，并将其与现有的机器学习框架集成，完成了微调算法轻量化存储的最后一里路。在Llama系列等多种大型语言模型及广泛使用的下游任务中的评估表明，所提出的HyC-LoRA框架相比基线方法实现了最高3.97倍的内存节省，且精度损失可忽略不计。
  
  